Due to getting closer to the US election and the rulings against Donald Trump, I've thought quite a lot about fringe beliefs. Often we hear that fringe beliefs or conspiracy theories are due to stupidity or a lack of education combined with misinformation. I think this is partly true, but I also believe that it also depends on today's information environment. My model of belief acquisition is inspired by [Schwartzstein & Sunderam (2019)](https://www.aeaweb.org/articles?id=10.1257/aer.20191074). In the paper, they model a situation where a Bayesian agent is persuaded by a model used to explain some data.  

The basic setup of their model is that a Sender agent and Receiver agent both see the same data, but they interpret the data in different ways. The interpretation is done through a model. The Sender, who wants the Receiver to take a specific action, can use their model to persuade the Receiver into a belief. Belief acquisition happens whenever the proposed model fits the data better than the Receiver's initial model. After acquiring the belief, it is optimal for the Bayesian to take the action the Sender wants. 

The model, under some simplifying assumptions, shows us that there are three factors that affect the likelihood of the Sender persuading the Receiver with her model. (1) How difficult it is to explain the data under the default model, i.e how well the default model fits the data, (2) how restrictive the Receiver's prior over world states is, and (3) how ambiguous the data is. 

Let's investigate this result in the context of the real world. (1) implies that Receivers are more likely to acquire beliefs through model persuasion if they are poor at coming up with models that fit the data well themselves. This means that we should expect expert agents, such as political pundits, researchers, and other experts to be the main persuaders. We should also expect non-experts to be more susceptible to persuasion. 

Similarly, (2) also implies that non-experts are more susceptible to persuasion. That is, a more restrictive prior over world states likely depends on expert knowledge. This knowledge allows agents to have a "finer" prior distribution. An extreme example is that a physicist can exclude physically impossible scenarios from their prior distribution, while a layman with little to no knowledge of physics can have some probability mass on physically impossible scenarios such as faster than light travel. 

Looking at the evidence from the real world, it is clear that experts are the main persuaders in our society (probably not only due to having an advantage in model-creation). It also seems to be the case that people with less education are more susceptible to persuasion. Though there are some counterexamples, such as a high share of college graduates being part of Jan 6th in the US.  

However, the factor that affects us the most, I believe, is (3). In the paper, it is assumed that both the Receiver and the Sender observe the same data—based on this, (3) is derived. But in the real world, the situation is even more ambiguous as there is so much data and information that any one person cannot observe all of it. Then the Sender has the potential to _choose_ which data to present to the Receiver. To maximize the likelihood of persuasion, the Sender should choose maximally ambiguous data, which allows her to come up with models that fit the data very well compared to almost any default model. 

Based on the above three factors, we should expect anyone trying to persuade others of "fringe" beliefs to (a) have some domain expertise (or at least give the impression of having it),  (b) hinge their models on very ambiguous data that is hard to explain in simpler default models, and (c) choose the data they present based on the model they would like to persuade listeners of. 

I would argue that this is exactly what we see in much of belief-acquisition today. The easiest examples are covid-beliefs, where "experts" try to persuade their audience that covid was due to a lab-leak. In this case, the persuaders often highlight the data related to Dr. Fauci, the actions of the NIH, and the lab in Wuhan—while ignoring data about e.g the likelihood of a natural evolution, wet markets, and similar. For Receivers, this means comparing their default model to the proposed model on data that is ambiguous which is hard for them to explain under the default—leading to a high likelihood of being persuaded. (The reason I'm using covid as an example is that the recent Fauci hearing has led to another wave of persuasion about the lab-leak hypothesis. My current belief is that a lab-leak is unlikely (<10% probability), see for example, [the Rootclaim debate on covid origins](https://www.youtube.com/watch?v=Y1vaooTKHCM&t=3909s)).

The antidotes to persuasion is given by the above factors as well. By ingesting more information, the default model can be made sharper and less prone to being overpowered by new ambiguous data. Learning can also make the prior distribution over world states sharper, which reduces the likelihood of persuasion. 

I don't want to give the impression that model persuasion alone explains everything about beliefs and belief-acquisition. I also believe that there's a social status component which has an effect. 